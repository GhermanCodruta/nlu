{
 "cells": [
  {
   "cell_type": "heading",
   "metadata": {
    "collapsed": true
   },
   "level": 1,
   "source": [
    "Prediction process for new utterances. Live action of the app."
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "1. Load the word vectors along with the data about the intents and slots."
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "1.1 In order to do this we need to import the modules of the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nWARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\nFor more information, please see:\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n  * https://github.com/tensorflow/addons\nIf you depend on functionality not listed there, please file an issue.\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import data_loader\n",
    "import model\n",
    "import main_module\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the *data_loader* module we load the training/ test data, word vectors and extract the intents and slot types."
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Next we need to pad the sentence to the length of the longest sentence in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------read datasets begin-------------------\n------------------load word2vec begin-------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------load word2vec end---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------read datasets end---------------------\n"
     ]
    }
   ],
   "source": [
    "data = data_loader.read_datasets()"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Now that we have the data set up, we need to set the flags for the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load settings\n",
    "vocab_size, word_emb_size = data['embedding'].shape\n",
    "_, max_sentence_length = data['x_tr'].shape\n",
    "intents_number = len(data['intents_dict'])\n",
    "slots_number = len(data['slots_dict'])\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_float(\"keep_prob\", 0.8, \"embedding dropout keep rate\")\n",
    "tf.app.flags.DEFINE_integer(\"hidden_size\", 32, \"embedding vector size\")\n",
    "tf.app.flags.DEFINE_integer(\"batch_size\", 1, \"vocab size of word vectors\")\n",
    "tf.app.flags.DEFINE_integer(\"num_epochs\", 20, \"num of epochs\")\n",
    "tf.app.flags.DEFINE_integer(\"vocab_size\", vocab_size, \"vocab size of word vectors\")\n",
    "tf.app.flags.DEFINE_integer(\"max_sentence_length\", max_sentence_length, \"max number of words in one sentence\")\n",
    "tf.app.flags.DEFINE_integer(\"intents_nr\", intents_number, \"intents_number\")  #\n",
    "tf.app.flags.DEFINE_integer(\"slots_nr\", slots_number, \"slots_number\")  #\n",
    "tf.app.flags.DEFINE_integer(\"word_emb_size\", word_emb_size, \"embedding size of word vectors\")\n",
    "tf.app.flags.DEFINE_boolean(\"use_embedding\", True, \"whether to use embedding or not.\")\n",
    "tf.app.flags.DEFINE_float(\"learning_rate\", 0.01, \"learning rate\")\n",
    "tf.app.flags.DEFINE_float(\"margin\", 1.0, \"ranking loss margin\")\n",
    "tf.app.flags.DEFINE_integer(\"slot_routing_num\", 4, \"slot routing num\")\n",
    "tf.app.flags.DEFINE_integer(\"intent_routing_num\", 4, \"intent routing num\")\n",
    "tf.app.flags.DEFINE_integer(\"re_routing_num\", 3, \"re routing num\")\n",
    "tf.app.flags.DEFINE_integer(\"intent_output_dim\", 64, \"intent output dimension\")\n",
    "tf.app.flags.DEFINE_integer(\"slot_output_dim\", 128, \"slot output dimension\")\n",
    "tf.app.flags.DEFINE_integer(\"attention_output_dimenison\", 20, \"self attention weight hidden units number\")\n",
    "tf.app.flags.DEFINE_float(\"alpha\", 0.001, \"coefficient for self attention loss\")\n",
    "tf.app.flags.DEFINE_integer(\"r\", 3, \"self attention weight hops\")\n",
    "tf.app.flags.DEFINE_boolean(\"save_model\", False, \"save model to disk\")\n",
    "tf.app.flags.DEFINE_boolean(\"test\", False, \"Evaluate model on test data\")\n",
    "tf.app.flags.DEFINE_boolean(\"crossval\", False, \"Perform k-fold cross validation\")\n",
    "tf.app.flags.DEFINE_integer(\"n_splits\", 3, \"Number of cross-validation splits\")\n",
    "tf.app.flags.DEFINE_string(\"summaries_dir\", './logs', \"tensorboard summaries\")\n",
    "tf.app.flags.DEFINE_string(\"ckpt_dir\", './saved_models/Scenario0', \"check point dir\")\n",
    "tf.app.flags.DEFINE_string(\"scenario_num\", '', \"Scenario number\")\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "This will set the main flags of the application, these flags are hardcoded in the *setting()* method from the main module."
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Input sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fa mai mica intensitatea iluminatului in sufragerie la 4\n"
     ]
    }
   ],
   "source": [
    "input_sentence = 'fa mai mica intensitatea iluminatului in sufragerie la 4'\n",
    "print(input_sentence)"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Transform sentence words into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fa mai mica intensitatea iluminatului in sufragerie la 4\nba\n[11165, 23, 5076, 10840, 66383, 50, 30878, 8, 129]\n"
     ]
    }
   ],
   "source": [
    "print(input_sentence)\n",
    "print(\"ba\")\n",
    "sentence_words = input_sentence.split(' ')\n",
    "encoded_words = []\n",
    "for w in sentence_words:\n",
    "    encoded_words.append(data_loader.w2v_dict['w2v'].vocab[w].index)\n",
    "    \n",
    "print(encoded_words)"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Next we need to pad the sentence to the length of the longest sentence in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "max_len = data['max_len']\n",
    "print(len(encoded_words))\n",
    "encoded_padded_words = np.append(encoded_words, np.zeros((max_len - len(encoded_words),), dtype=np.int64))\n",
    "\n",
    "data['sample_utterance'] = encoded_padded_words"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Next we need to load the saved model from the checkpoint directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Projects\\nlu\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Projects\\nlu\\tb-capsnet-architecture\\model.py:114: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Projects\\nlu\\tb-capsnet-architecture\\model.py:122: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Projects\\nlu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Projects\\nlu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Projects\\nlu\\tb-capsnet-architecture\\model.py:220: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Projects\\nlu\\tb-capsnet-architecture\\model.py:186: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\nInstructions for updating:\ndim is deprecated, use axis instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Projects\\nlu\\tb-capsnet-architecture\\model.py:168: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring Variables from Checkpoint for testing\nWARNING:tensorflow:From C:\\Projects\\nlu\\venv\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saved_models/Scenario0\\model.ckpt-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utterance classified as: scadeIntensitateLumina\nSlots:\n['O', 'O', 'O', 'O', 'O', 'O', 'B-loc', 'O', 'O', 'O', 'O', 'O', 'O', 'B-loc', 'O']\nFinished inferring\n['O', 'O', 'O', 'O', 'O', 'O', 'B-loc', 'O', 'O', 'O', 'O', 'O', 'O', 'B-loc', 'O']\n2\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "with tf.Session(config=config) as sess:\n",
    "    # Instantiate Model\n",
    "    capsnet = model.capsnet(FLAGS)\n",
    "    if os.path.exists(FLAGS.ckpt_dir):\n",
    "        print(\"Restoring Variables from Checkpoint for testing\")\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n",
    "        slots, intents = main_module.evaluate_sample(capsnet, data, sess)\n",
    "    else:\n",
    "        print(\"No trained model exists in checkpoint dir!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'slots' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4d2aaf084554>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslots\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'slots' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
