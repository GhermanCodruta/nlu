{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Prediction process for new utterances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the environment\n",
    "#### 1. In order to do this we need to import the modules of the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import data_loader\n",
    "import model\n",
    "import main_module\n",
    "import numpy as np\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR) # removes the warning messeges that pollute the console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. To set the basis for prediction we need to encode words into vectors, we can't perform computations on vectors.\n",
    "Moreover, we need the intents and slot types defined as well. These informations are extracted from the dataset on which the model was trained.\n",
    "\n",
    "The data_loader module loads the word vectors into memory (huge dictionary of words as keys and vectors as values). Along with loading the word vectors it encodes the training and testing data as well. While encoding the data it also extracts the possible intents and slot types and assigns them ids. \n",
    "\n",
    "###### This step takes the most time, on my machine it takes about 10 minutes to load the word vectors into memory, about 4 gb with a speed of 6 - 8 mb/s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------read datasets begin-------------------\n",
      "------------------load word2vec begin-------------------\n",
      "------------------load word2vec end---------------------\n",
      "------------------read datasets end---------------------\n"
     ]
    }
   ],
   "source": [
    "data = data_loader.read_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Now that we have the data set up, next we need to set the flags for the architecture. \n",
    "Tensorflow permits working with flags helping with the processing of command line parameters as well as generally providing a way to better organize the variable parameters to an application.\n",
    "\n",
    "Some of these flags are computed from the given data, others are assigned by us with values obtianed in the hyperparameter tuning process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load settings\n",
    "vocab_size, word_emb_size = data['embedding'].shape\n",
    "_, max_sentence_length = data['x_tr'].shape\n",
    "intents_number = len(data['intents_dict'])\n",
    "slots_number = len(data['slots_dict'])\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_float(\"keep_prob\", 0.8, \"embedding dropout keep rate\")\n",
    "tf.app.flags.DEFINE_integer(\"hidden_size\", 32, \"embedding vector size\")\n",
    "tf.app.flags.DEFINE_integer(\"batch_size\", 1, \"vocab size of word vectors\")\n",
    "tf.app.flags.DEFINE_integer(\"num_epochs\", 20, \"num of epochs\")\n",
    "tf.app.flags.DEFINE_integer(\"vocab_size\", vocab_size, \"vocab size of word vectors\")\n",
    "tf.app.flags.DEFINE_integer(\"max_sentence_length\", max_sentence_length, \"max number of words in one sentence\")\n",
    "tf.app.flags.DEFINE_integer(\"intents_nr\", intents_number, \"intents_number\")  #\n",
    "tf.app.flags.DEFINE_integer(\"slots_nr\", slots_number, \"slots_number\")  #\n",
    "tf.app.flags.DEFINE_integer(\"word_emb_size\", word_emb_size, \"embedding size of word vectors\")\n",
    "tf.app.flags.DEFINE_boolean(\"use_embedding\", True, \"whether to use embedding or not.\")\n",
    "tf.app.flags.DEFINE_float(\"learning_rate\", 0.01, \"learning rate\")\n",
    "tf.app.flags.DEFINE_float(\"margin\", 1.0, \"ranking loss margin\")\n",
    "tf.app.flags.DEFINE_integer(\"slot_routing_num\", 4, \"slot routing num\")\n",
    "tf.app.flags.DEFINE_integer(\"intent_routing_num\", 4, \"intent routing num\")\n",
    "tf.app.flags.DEFINE_integer(\"re_routing_num\", 3, \"re routing num\")\n",
    "tf.app.flags.DEFINE_integer(\"intent_output_dim\", 64, \"intent output dimension\")\n",
    "tf.app.flags.DEFINE_integer(\"slot_output_dim\", 128, \"slot output dimension\")\n",
    "tf.app.flags.DEFINE_integer(\"attention_output_dimenison\", 20, \"self attention weight hidden units number\")\n",
    "tf.app.flags.DEFINE_float(\"alpha\", 0.001, \"coefficient for self attention loss\")\n",
    "tf.app.flags.DEFINE_integer(\"r\", 3, \"self attention weight hops\")\n",
    "tf.app.flags.DEFINE_boolean(\"save_model\", False, \"save model to disk\")\n",
    "tf.app.flags.DEFINE_boolean(\"test\", False, \"Evaluate model on test data\")\n",
    "tf.app.flags.DEFINE_boolean(\"crossval\", False, \"Perform k-fold cross validation\")\n",
    "tf.app.flags.DEFINE_integer(\"n_splits\", 3, \"Number of cross-validation splits\")\n",
    "tf.app.flags.DEFINE_string(\"summaries_dir\", './logs', \"tensorboard summaries\")\n",
    "tf.app.flags.DEFINE_string(\"ckpt_dir\", './saved_models/Scenario0', \"check point dir\")\n",
    "tf.app.flags.DEFINE_string(\"scenario_num\", '', \"Scenario number\")\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')  # required by jupyter notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Load the model from the checkpoint directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Projects\\nlu\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Projects\\nlu\\tb-capsnet-architecture\\model.py:114: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From C:\\Projects\\nlu\\tb-capsnet-architecture\\model.py:122: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Projects\\nlu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Projects\\nlu\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Projects\\nlu\\tb-capsnet-architecture\\model.py:220: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Projects\\nlu\\tb-capsnet-architecture\\model.py:186: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "WARNING:tensorflow:From C:\\Projects\\nlu\\tb-capsnet-architecture\\model.py:168: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Restoring Variables from Checkpoint for testing\n",
      "WARNING:tensorflow:From C:\\Projects\\nlu\\venv\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./saved_models/Scenario0\\model.ckpt-1\n",
      "Utterance classified as: scadeIntensitateLumina\n",
      "Slots:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-loc', 'O', 'O', 'O', 'O', 'O', 'O', 'B-loc', 'O']\n",
      "Finished inferring\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-loc', 'O', 'O', 'O', 'O', 'O', 'O', 'B-loc', 'O']\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "sess = tf.Session(config=config) \n",
    "# Instantiate Model\n",
    "capsnet = model.capsnet(FLAGS)\n",
    "if os.path.exists(FLAGS.ckpt_dir):\n",
    "    print(\"Restoring Variables from Checkpoint for testing\")\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n",
    "else:\n",
    "    print(\"No trained model exists in checkpoint dir!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Now that we have everything loaded, the only thing left is to input the new data samples to be predicted. \n",
    "The next cells have the responsibility to prepare the new data in the right format for prediction.\n",
    "   \n",
    "Important guidelines are that the intents and slots should be from the set of predefined ones found in the training data. Moreover, the utterance has must not exceed the length of the longest utterance seen in the data set. In the following we will address this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fa mai mica intensitatea iluminatului in sufragerie la 4\n"
     ]
    }
   ],
   "source": [
    "input_sentence = 'fa mai mica intensitatea iluminatului in sufragerie la 4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. We can't perform computations on words in text hence we need to transform the input sentence into words vectors, using the same word vectors used to encode the trainind data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fa mai mica intensitatea iluminatului in sufragerie la 4\n",
      "ba\n",
      "[11165, 23, 5076, 10840, 66383, 50, 30878, 8, 129]\n"
     ]
    }
   ],
   "source": [
    "sentence_words = input_sentence.split(' ')\n",
    "encoded_words = []\n",
    "for w in sentence_words:\n",
    "    encoded_words.append(data_loader.w2v_dict['w2v'].vocab[w].index)\n",
    "    \n",
    "print(encoded_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Next we need to pad the sentence to the length of the longest sentence in the training set.\n",
    "(weird constraint, but the motivation behind it is that each utterance has a random length which is a porblem because we would like stitch multiple utterances together into matrices and perform faster computations on them. Different lengths are an abonimation in matrices so we decided to pad them, but how long should the sentences be? If too long then we perform a lot of computations for no reason, if too short we risk loosing relevant informations from sentences since we would need to trim --> this led us to the trade off to pad every sentence to be as long as the longest one in the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "max_len = data['max_len']\n",
    "print(\"original sentencce length\" + len(encoded_words))\n",
    "encoded_padded_words = np.append(encoded_words, np.zeros((max_len - len(encoded_words),), dtype=np.int64))\n",
    "data['sample_utterance'] = encoded_padded_words\n",
    "print(\"padded sentence length\" + len(encoded_padded_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Finally we can perform the prediction using the loaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slots, intents = main_module.evaluate_sample(capsnet, data, sess)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
