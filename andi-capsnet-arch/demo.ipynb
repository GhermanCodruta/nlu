{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CapsNetS2I - Capsule Neural Network Architecture for Joint Intent Detection and Slot Filling\n",
    "\n",
    "<img src=\"high-level-arch.png\" width=\"50%\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary modules and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import model\n",
    "import data_loader\n",
    "import flags\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "First, we define the paths for the pre-trained word embeddings, train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = '../data-capsnets/word-vec/cc.ro.300.vec'\n",
    "training_data_path = '../data-capsnets/scenario0/train.txt'\n",
    "test_data_path = '../data-capsnets/scenario0/test.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pre-trained word embeddings\n",
    "There are a total of ... word vectors, so loading these takes a while (~ 10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------------load word2vec begin-------------------')\n",
    "w2v = data_loader.load_w2v(word2vec_path)\n",
    "print('------------------load word2vec end---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------read datasets begin-------------------\n",
      "------------------read datasets end---------------------\n"
     ]
    }
   ],
   "source": [
    "data = data_loader.read_input_data(w2v, training_data=training_data_path, test_data=test_data_path, test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the contents of the data dictionary\n",
    "\n",
    "`intents_dict` - maps ids to intent labels\n",
    "<br>\n",
    "`slots_dict` - maps ids to slot labels\n",
    "<br>\n",
    "For slots we use the **IOB** (Inside, Outside, Beginning) notation - useful for slots that span multiple words (i.e. _\"douazeci de grade\"_ --> B-grade I-grade I-grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent class dictionary\n",
      "{   0: 'stingeLumina',\n",
      "    1: 'seteazaTemperatura',\n",
      "    2: 'scadeIntensitateLumina',\n",
      "    3: 'schimbaCanalTV',\n",
      "    4: 'schimbaIntensitateMuzica',\n",
      "    5: 'opresteMuzica',\n",
      "    6: 'opresteTV',\n",
      "    7: 'cresteIntensitateLumina',\n",
      "    8: 'pornesteTV',\n",
      "    9: 'scadeTemperatura',\n",
      "    10: 'puneMuzica',\n",
      "    11: 'cresteTemperatura',\n",
      "    12: 'aprindeLumina'}\n",
      "Slots class dictionary\n",
      "{   0: 'O',\n",
      "    1: 'B-loc',\n",
      "    2: 'B-grade',\n",
      "    3: 'I-grade',\n",
      "    4: 'B-nivel',\n",
      "    5: 'B-canal',\n",
      "    6: 'B-artist',\n",
      "    7: 'I-loc'}\n",
      "Max sentence length: 15 words\n",
      "\n",
      "Test sample\n",
      "['Salut' 'as' 'vrea' 'sa' 'maresti' 'intensitatea' 'luminii' 'in'\n",
      " 'pivnita' '0' '0' '0' '0' '0' '0']\n",
      "Intent: 7\n",
      "Slots: [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "Sample containing word embedding indices:\n",
      "[ 13871   1949    540     59 440964  10840   5334     50 215040      0\n",
      "      0      0      0      0      0]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "print('Intent class dictionary')\n",
    "pp.pprint(data['intents_dict'])\n",
    "print('Slots class dictionary')\n",
    "pp.pprint(data['slots_dict'])\n",
    "print('Max sentence length: %d words\\n' % data['max_len'])\n",
    "\n",
    "test_sample_idx = 110\n",
    "print('Test sample')\n",
    "print(data['x_text_te'][test_sample_idx])\n",
    "print('Intent: %s' % data['y_intents_te'][test_sample_idx])\n",
    "print('Slots: %s' % data['y_slots_te'][test_sample_idx])\n",
    "print('Sample containing word embedding indices:')\n",
    "print(data['x_te'][test_sample_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set TensorFlow application flags\n",
    "These flags contain application-wide information such as the scenario, the checkpoint directory containing the saved model, as well as the hyperparameters of the model: learning rate, batch size, number of epochs, the dimensionality of the prediction and \n",
    "output vectors of the capsule neural network model, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, word_emb_size = data['embedding'].shape\n",
    "_, max_sentence_length = data['x_tr'].shape\n",
    "intents_number = len(data['intents_dict'])\n",
    "slots_number = len(data['slots_dict'])\n",
    "hidden_size = 64\n",
    "    \n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('ckpt_dir', './saved_models/', 'check point dir')\n",
    "tf.app.flags.DEFINE_string('scenario_num', '0', 'Scenario number')\n",
    "tf.app.flags.DEFINE_string('errors_dir', './errors/', 'Errors dir')\n",
    "tf.app.flags.DEFINE_float('keep_prob', 0.8, 'embedding dropout keep rate for training')\n",
    "tf.app.flags.DEFINE_integer('hidden_size', hidden_size, 'embedding vector size')\n",
    "tf.app.flags.DEFINE_integer('batch_size', 32, 'batch size')\n",
    "tf.app.flags.DEFINE_integer('num_epochs', 20, 'num of epochs')\n",
    "tf.app.flags.DEFINE_integer('vocab_size', vocab_size, 'vocab size of word vectors')\n",
    "tf.app.flags.DEFINE_integer('max_sentence_length', max_sentence_length, 'max number of words in one sentence')\n",
    "tf.app.flags.DEFINE_integer('intents_nr', intents_number, 'intents_number')  #\n",
    "tf.app.flags.DEFINE_integer('slots_nr', slots_number, 'slots_number')  #\n",
    "tf.app.flags.DEFINE_integer('word_emb_size', word_emb_size, 'embedding size of word vectors')\n",
    "tf.app.flags.DEFINE_boolean('use_embedding', True, 'whether to use embedding or not.')\n",
    "tf.app.flags.DEFINE_float('learning_rate', 0.01, 'learning rate')\n",
    "tf.app.flags.DEFINE_integer('slot_routing_num', 2, 'slot routing num')\n",
    "tf.app.flags.DEFINE_integer('intent_routing_num', 3, 'intent routing num')\n",
    "tf.app.flags.DEFINE_integer('intent_output_dim', 16, 'intent output dimension')\n",
    "tf.app.flags.DEFINE_integer('slot_output_dim', 2 * hidden_size, 'slot output dimension')\n",
    "tf.app.flags.DEFINE_integer('d_a', 20, 'self attention weight hidden units number')\n",
    "tf.app.flags.DEFINE_integer('r', 5, 'number of self attention heads')\n",
    "tf.app.flags.DEFINE_float('alpha', 0.0001, 'coefficient for self attention loss')\n",
    "tf.app.flags.DEFINE_integer('n_splits', 3, 'Number of cross-validation splits')\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reset flags\n",
    "We should run this whenever we want to make a change in one flag, as redefinition of an existing flag is not possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in list(FLAGS):\n",
    "  delattr(FLAGS, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load trained model from checkpoint directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring Variables from Checkpoint for testing\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "# Instantiate Model\n",
    "capsnet = model.CapsNet(FLAGS)\n",
    "\n",
    "ckpt_dir = FLAGS.ckpt_dir + 'scenario' + FLAGS.scenario_num + '/'\n",
    "if os.path.exists(ckpt_dir):\n",
    "    print('Restoring Variables from Checkpoint for testing')\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(ckpt_dir))\n",
    "else:\n",
    "    print('No trained model exists in checkpoint dir!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence with w2v indices:\n",
      "[ 6511 63018  1949   540    59 88692  1639    50 15079]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "text = 'Buna Pepper as vrea sa aprinzi lumina in hol'\n",
    "text_len = np.asarray(len(text.split(' ')))\n",
    "max_len = data['max_len']\n",
    "\n",
    "# Map words to their corresponding word embedding indices\n",
    "text_vec = []\n",
    "for w in text.split(' '):\n",
    "    if w in w2v.vocab:\n",
    "        text_vec.append(w2v.vocab[w].index)\n",
    "    else:\n",
    "        print('Word %s not in vocabulary!' % w)\n",
    "        break\n",
    "text_vec = np.asarray(text_vec)\n",
    "print('Sentence with w2v indices:')\n",
    "print(text_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded sentence:\n",
      "[ 6511 63018  1949   540    59 88692  1639    50 15079     0     0     0\n",
      "     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "# Pad sentence with 0s so that it fills the maximum sentence length\n",
    "text_vec_pad = np.append(text_vec, np.zeros((max_len - text_len,), dtype=np.int64))\n",
    "print('Padded sentence:')\n",
    "print(text_vec_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand dimensions of the input data to fit the dimensions that the model expects:\n",
    "<br>\n",
    "`input_x` - shape (batch_size, max_len)\n",
    "<br>\n",
    "`sentence_len` - shape (batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_vec_pad shape: (15,)\n",
      "text_vec_pad_dim shape: (1, 15)\n",
      "text_len shape: ()\n",
      "text_len_dim shape: (1,)\n"
     ]
    }
   ],
   "source": [
    "print('text_vec_pad shape: ' + str(text_vec_pad.shape))\n",
    "text_vec_pad_dim = np.expand_dims(text_vec_pad, axis=0)\n",
    "print('text_vec_pad_dim shape: ' + str(text_vec_pad_dim.shape))\n",
    "\n",
    "print('text_len shape: ' + str(text_len.shape))\n",
    "text_len_dim = np.expand_dims(text_len, axis=0)\n",
    "print('text_len_dim shape: ' + str(text_len_dim.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "\n",
    "# Feed the sample to the model to obtain slot and intent predictions\n",
    "[intent_outputs, slots_outputs, slot_weights_c] = sess.run([\n",
    "            capsnet.intent_output_vectors, capsnet.slot_output_vectors, capsnet.slot_weights_c],\n",
    "            feed_dict={capsnet.input_x: text_vec_pad_dim, capsnet.sentences_length: text_len_dim,\n",
    "                       capsnet.keep_prob: 1.0})\n",
    "\n",
    "intent_outputs_reduced_dim = tf.squeeze(intent_outputs, axis=[1, 4])\n",
    "intent_outputs_norm = util.safe_norm(intent_outputs_reduced_dim)\n",
    "slot_weights_c_reduced_dim = tf.squeeze(slot_weights_c, axis=[3, 4])\n",
    "\n",
    "[intent_predictions, slot_predictions] = sess.run([intent_outputs_norm, slot_weights_c_reduced_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the **13 IntentCaps** output a **16-dimensional vector**.\n",
    "<br>\n",
    "For slot filling, we are interested in the **routing weights** between **WordCaps** and **SlotCaps** -- in this way we predict the slot type of each individual word. There is a routing weight associated to each word capsule - slot capsule pair (15 x 8 total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IntentCaps output shape: (1, 1, 13, 16, 1)\n",
      "WordCaps - SlotCaps routing weights: (1, 15, 8, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "print('IntentCaps output shape: ' + str(intent_outputs.shape))\n",
    "print('WordCaps - SlotCaps routing weights: ' + str(slot_weights_c.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buna Pepper as vrea sa aprinzi lumina in hol\n",
      "Intent prediction: aprindeLumina\n",
      "Slots prediction: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-loc']\n"
     ]
    }
   ],
   "source": [
    "intent_pred = np.argmax(intent_predictions, axis=1)\n",
    "slots_pred = np.argmax(slot_predictions, axis=2)\n",
    "\n",
    "intent_label = data['intents_dict'][intent_pred[0]]\n",
    "slot_labels = [data['slots_dict'][x] for x in slots_pred[0]]\n",
    "\n",
    "print(text)\n",
    "print('Intent prediction: ' + intent_label)\n",
    "print('Slots prediction: ' + str(slot_labels[:text_len]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Da drumul la lumina in hol\n",
      "Intent prediction: cresteIntensitateLumina\n",
      "Slots prediction: ['O', 'O', 'O', 'O', 'O', 'B-loc']\n"
     ]
    }
   ],
   "source": [
    "text = 'Da drumul la lumina in hol'\n",
    "\n",
    "text_len = np.asarray(len(text.split(' ')))\n",
    "max_len = data['max_len']\n",
    "\n",
    "# Map words to their corresponding word embedding indices\n",
    "text_vec = []\n",
    "for w in text.split(' '):\n",
    "    if w in w2v.vocab:\n",
    "        text_vec.append(w2v.vocab[w].index)\n",
    "    else:\n",
    "        print('Word %s not in vocabulary!' % w)\n",
    "        break\n",
    "text_vec = np.asarray(text_vec)\n",
    "\n",
    "# Pad sentence with 0s so that it fills the maximum sentence length\n",
    "text_vec_pad = np.append(text_vec, np.zeros((max_len - text_len,), dtype=np.int64))\n",
    "text_vec_pad_dim = np.expand_dims(text_vec_pad, axis=0)\n",
    "text_len_dim = np.expand_dims(text_len, axis=0)\n",
    "\n",
    "# Feed the sample to the model to obtain slot and intent predictions\n",
    "[intent_outputs, slots_outputs, slot_weights_c] = sess.run([\n",
    "            capsnet.intent_output_vectors, capsnet.slot_output_vectors, capsnet.slot_weights_c],\n",
    "            feed_dict={capsnet.input_x: text_vec_pad_dim, capsnet.sentences_length: text_len_dim,\n",
    "                       capsnet.keep_prob: 1.0})\n",
    "\n",
    "intent_outputs_reduced_dim = tf.squeeze(intent_outputs, axis=[1, 4])\n",
    "intent_outputs_norm = util.safe_norm(intent_outputs_reduced_dim)\n",
    "slot_weights_c_reduced_dim = tf.squeeze(slot_weights_c, axis=[3, 4])\n",
    "\n",
    "[intent_predictions, slot_predictions] = sess.run([intent_outputs_norm, slot_weights_c_reduced_dim])\n",
    "\n",
    "intent_pred = np.argmax(intent_predictions, axis=1)\n",
    "slots_pred = np.argmax(slot_predictions, axis=2)\n",
    "\n",
    "intent_label = data['intents_dict'][intent_pred[0]]\n",
    "slot_labels = [data['slots_dict'][x] for x in slots_pred[0]]\n",
    "\n",
    "print(text)\n",
    "print('Intent prediction: ' + intent_label)\n",
    "print('Slots prediction: ' + str(slot_labels[:text_len]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
